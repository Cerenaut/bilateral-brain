seeds: [4]    # it is possible to do multiple seeds (if there is only 1, it must still be in an array)
evaluate: False

trainer_params:
  accelerator: null
  gpus: 0
  default_root_dir: ../runs  # root folder for pytorch-lightning (but logs and checkpoints go to the logger save_dir)
  max_epochs: 100
  log_every_n_steps: 10
  accumulate_grad_batches: 2

dataset:
  train_dir: ../datasets/CIFAR100/train/coarse
  val_dir: ../datasets/CIFAR100/test/coarse
  test_dir: ../datasets/CIFAR100/test/coarse
  raw_data_dir: ../../datasets/cifar-100-python/  # needs a trailing slash

logger:
  type: TensorBoardLogger
  save_dir: ./logs/         # logs and checkpoints go here
  name: logger_name         # customize the name to the run you are doing
  version: layer=only1|lr=1.0e-4|wd=1.0e-5|

hparams:
  k: 0.9
  lr: 1.0e-4
  weight_decay: 1.0e-5
  optimizer: adam
  warmup_epochs: 0
  batch_size: 256
  num_workers: 4
  broad_k: 0.9
  broad_per_k: 0.9
  narrow_k: 0.9
  narrow_per_k: 0.9
  model_path_fine: /home/yanfeng/Dev/bilateral-brain/classification/logs_fine/logger_name-seed4/lr=0.0001|opt=adam|/checkpoints/epoch=85-val_acc=0.61.ckpt  # path to narrow model checkpoint
  model_path_broad: /home/yanfeng/Dev/bilateral-brain/classification/logs_coarse/logger_name-seed4/lr=0.0001|opt=adam|/checkpoints/epoch=89-val_acc=0.74.ckpt  # path to broad model checkpoint

ckpt_callback:
  save_top_k: 2
  monitor: val_loss
  mode: min
  save_last: True