seeds: [4]    # it is possible to do multiple seeds (if there is only 1, it must still be in an array)
evaluate: True
save_dir: ./runs/   # exp output folder: logs, checkpoints, hparams and accuracy
exp_name: dual_head

trainer_params:
  accelerator: gpu
  devices: auto
  default_root_dir: ./runs  # root folder for pytorch-lightning (but logs and checkpoints go to the logger save_dir)
  max_epochs: 2
  log_every_n_steps: 10
  accumulate_grad_batches: 2

dataset:
  train_dir: ../datasets/CIFAR100/train_small/fine
  val_dir: ../datasets/CIFAR100/test_small/fine
  test_dir: ../datasets/CIFAR100/test_small/fine
  raw_data_dir: ../../datasets/cifar-100-python/  # needs a trailing slash

logger:
  type: TensorBoardLogger

hparams:
  mode: 'both'              # where to get output from:     [both | feature]
  macro_arch: 'unilateral'  # unilateral or bilateral. If unilateral, use farch
  farch: 'sparse_resnet9'   # architecture for 'fine' learning  [resnet9 | sparse_resnet9 | inverted_resnet9, resnet_x, vgg11] 
  carch: ''                 # architecture for 'coarse' learning e.g. resnet9. THIS IS IGNORED if macro_arch is 'unilateral'
  ffreeze: False
  cfreeze: False
  lr: 1.0e-4
  weight_decay: 1.0e-5
  warmup_epochs: 0
  batch_size: 256
  num_workers: 4
  fine_k: 0.0
  fine_per_k: 0.0
  coarse_k: 0.0
  coarse_per_k: 0.0
  # model_path_fine: path_to_checkpoint  # path to narrow model checkpoint
  # model_path_coarse: path_to_checkpoint  # path to broad model checkpoint

ckpt_callback:
  save_top_k: 2
  monitor: val_loss
  mode: min
  save_last: True